<!DOCTYPE html>
<html>
<head>
    <title>Lateral Inhibition in CNN</title>
</head>
<body>
<h2>Modeling Lateral Inhibition in Convolutional Neural Networks</h2>
<p style="margin-bottom: 5px;">
    Students: Quinton Denman, Julian Lehnen, Dávid Sebők , Tonio Weidler;
    Supervisors: Mario Senden, Gerhard Weiss, Kurt Driessens;
    Semester: 2019;
</p>

<figure style="float: left; width: min-content; height: min-content;">
    <img alt="" src="2019_Lehnen_img_1.png" style="width: 400px; height: 570px;">
    <figcaption>Fig 1. - Column B receives a strong excitation and inhibits the activity of columns A and C.
    </figcaption>
</figure>

<h4>Problem statement and motivation:</h4>
<p align="justify">
    A <b>Convolutional Neural Network</b> (CNN) mimics the way that a human brain processes visual information. Imagine
    a 5 year-old learning different animals. A picture of the animal is shown to them and they're told that it is, for
    example, a giraffe. Similarly, a CNN is shown many pictures and told that the image depicts a giraffe: the CNN is
    trained on labelled data. This can be extended to learn a lot more than this example, but the truth is, a CNN is
    only a crude copy of how the brain works. Neurons are modeled in a CNN using numerical values and weights which are
    then grouped into layers (called nodes). The typical structure of a neural network consists of an input layer (the
    values that make up an image), at least one hidden layer and an output layer. The nodes in the input layer influence
    the nodes in the next hidden layer and those nodes in the hidden layer influence the nodes in the output layer. This
    forms a network of connected nodes which (crudely) approximates the network of connected neurons present in the
    human brain. A CNN has a similar structure but contains at least one layer where the process of <i>convolution</i>
    is performed. Such layers employ so called <i>filters</i> that learn to respond to specific features within an image,
    for example lines at different angles.
</p>
<p align="justify">
    In contrast, the structure of the brain is extremely complicated but researchers in Neuroscience are constantly
    making strides in understanding its inner workings. The goal of our project is to take some of that new
    understanding and incorporate it into CNNs. By emulating the brain more closely we hope to improve CNNs (for
    example, it needing less images to learn how to distinguish between giraffes and elephants) or at least gain
    understanding about the brain's functioning by drawing conclusions from observations in the mathematical model.
</p>
<p align="justify">
    The specific concept we will be trying to replicate is called <b>Lateral Inhibition</b>. The brain has billions of
    neurons (brain cells) grouped into sectors with each sector handling different tasks. All the neurons responsible for
    handling visual input are located in one section of the brain, called the visual cortex, and were found to be
    arranged into six so called areas. In the first area, the primary visual cortex, neurons are organized in a series
    of columns. Some of these columns are responsible for identifying and processing different orientations of a line
    (for example, the difference between a straight line and a diagonal line) [Isaacson and Scanziani, 2011].
    Furthermore, all the columns that are responsible for this are lined up next to each other. They are organized based
    on the similarity of the orientations they represent (e.g. a column identifying a 90 degree line is next to a 95
    degree line and a 85 degree line). So when the eye sees a horizontal line, the column in the primary visual cortex
    representing that horizontal line activates. Additionally, these columns have the ability to influence their
    neighbours, either boosting the chance they fire or reducing it. For example, imagine three columns A, B and C (see
    Figure 1). The visual input relates to the orientation that column B represents. Column B then reduces or boosts the
    values of A and C. In return, A and C reduce or boost the value of column B. This effectively pushes or dimmes B's
    signal, ensuring that the activity in our brain more precisely fits what we actually see. In a CNN, the filters that
    search for features in the image, can be related to the columns that we just described.
</p>
<p align="justify">
    We hope that introducing the concept of lateral inhibition into the structure of CNNs will improve their performance
    and produce an ordered structure of filters, such as the one in the columns. We will then investigate how or if this
    is beneficial to the CNN. Since original CNNs do not have any connection between filters, their ordering is random.
    If introducing lateral inhibition causes this randomness to dissolve into some explainable structure, such an
    observation may shed further light onto the functioning of the columns in the brain. We would, for example, expect
    the order of filters to be based on the similarity of what they want to detect.
</p>
<p align="justify">
    To achieve this we propose three approaches that successively try to tackle their predecessors’ key problems. In our
    first approach we simply apply lateral inhibition as a single shot operation, altering the activity of the CNN in
    one step. However, in the brain, neuronal activity is an ongoing process, incorporating a time factor. In CNNs, we
    do not have such a factor of time, but rather one beginning-to-end calculation of an output. Problematically, the
    concept of lateral inhibition is heavily dependent on the time factor: If a column is inhibited, the effect it can
    later have on other columns will diminish and after a while all activity stabilizes. Our single-shot approach does
    not capture this characteristic. To account for this, our second idea is to repeat the step of inhibition for some
    number of steps, always applied on the result of the previous step. This hopefully brings the result closer to a
    stabilized state. However, this process is slow due to the repeated calculations and it is also still not exact. In
    our third approach we therefore attempt to mathematically determine the state of the neurons at the point at which
    they stabilize and then calculate this in one step.
</p>

<h4>Research questions:</h4>
<ul>
    <li>
        Does the proposed implementation of lateral inhibition improve performance or training speed on different tasks?
    </li>
    <li>
        How can lateral inhibition be implemented into convolutional neural networks?
    </li>
    <li>
        How do these different implementations benefit the neural networks?
    </li>
    <li>
        Will one of the proposed implementations result in a specific ordering of the CNN filters reminiscent of the
        ordering produced in the brain?
    </li>
    <li>
        If such an ordering can be observed, is it consistent over multiple training runs despite a varying random
        initialization of the nodes' values?
    </li>
</ul>

<h4>Main outcomes:</h4>
<p>
    In the end of this project we will deliver an elaborate study of our three approaches of integrating lateral
    inhibition into convolutional neural networks. We plan to provide the following results:
</p>
<ul>
    <li>
        We expect to show that the introduction of lateral inhibition imposes an ordering on the filters within the
        respective CNN.
    </li>
    <li>
        We hope to show that introducing lateral inhibition improves CNNs by utilising training data more efficiently,
        that is, given the same amount of training data a CNN with lateral inhibition provides a higher test accuracy
        than a CNN without it.
    </li>
</ul>

<h4>References:</h4>
Isaacson, Jeffry S. and Scanziani, Massimo (2011). How inhibition shapes cortical activity. Neuron 72(2), 231-243

<h4>Downloads:</h4>
<a href="" rel="noopener" target="_blank">Final report</a>
<a href="" rel="noopener" target="_blank">Final presentation</a>

</body>
</html>